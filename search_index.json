[["index.html", "SLP Notes 1 Notes", " SLP Notes brightertiger 1 Notes Hi I am brightertiger! I am a data scientist looking to expand my knowledge about ML. This is a collection of notes based mostly the book Speech and Language Processing. "],["regex.html", "2 Regex", " 2 Regex 2.0.1 Regex Language for specifying text search strings Algebraic notation for characterizing a set of strings Basic regular expression Match the “word” /word/ Match the “word” or “Word” /[wW]ord/ Match single digit /[1234567890]/ Ranges Capital Letters /[A-Z]/ Lower Case Letters /[a-z]/ Single Digit /[0-9]/ Caret Exclusions Not an upper case letter /[^A-Z]/ Not a period /[^.]/ If caret is not the first character, it’s treated as any other character Question Mark Preceding character or nothing “word” or “words” /words?/ “colour” or “color” /colou?r/ Kleene * Zero or more occurances Zero or more “a” /a*/ Zero or more “a”s or “b”s /[ab]*/ Kleene + One or more occurances One or more digits /[0-9]+/ Wildcard Match any single expression Any character between “beg” and “n” /beg.n/ Anchors Start of the line ^ Lines starting with “the” /^The/ End of the line $ Lines ending with period /\\.$/ Word boundary Grouping Disjunction “|” Match either cat or dog /cat|dog/ Paranthesis () Match “guppy” or “guppies” /gupp(y|ies)/ Example /(ˆ|[ˆa-zA-Z])[tT]he([ˆa-zA-Z]|$)/ At the start or a non-alphabetic character At the end or non-alphabetic character Look for “the” or “The” Operators Any digit Any non-digit Whitespace Non-whitespace Any alphanumeric Non Alpha-numeric Range Zero or more * One or more + Exactly zero or one ? N Occurances {n} N-to-M Occurances {n,m} Atleast N Occurances {n,} Upto M Occurances {,m} 2.0.2 Words Utterance is the spoken correlate of a sentence Disfluency Fragments: broken off words Fillers or Filled Pauses “um” Lemma: Lexical form of the same word (cats vs cat) Types (V): Number of distinct words Tokens (N): Number of running words Heap’s Law: \\(V = K N^\\beta\\) 2.0.3 Text Normalization Involves three steps Tokenzing Words Normalizing word formats Segmenting sentences Tokenization Breaing up a an utterance into tokens Penn Treebank Tokenization NLTK Regex Tokenization Byte Pair Encoding Emperically determine the tokens using data Useful in dealing with unseen words Use subwords tokens which are arbitrary substrings Token Learner: Creates vocabulary out of corpus Token Segementor: Applies token learner on raw test data BPE Token Learner Starts with individual characters as vocab Merges the most frequently occuring pairs and adds them to back vocab Repeats the count and merge process to create longer substrings Continues until vocab size is reached No merging across word boundries BPE Token Parser Run the token learner on test data Same order in which tokens were created First split into individual characters Merge the characters based on BPE vocab 2.0.4 Word Normalization Putting words and tokens in a standard format Case Folding: Convert everything to lowercase Lemmatization: Reduce words to roots Stemming (ing, ed etc.) Porter Stemming 2.0.5 Edit Distance Similarity between two strings Minimum number of editing operations needed to transform one string into another Insertion, Deletion and Substitution Levenstien Distance: All three operations ahve the same cost Dynamic Programming Viterbi Algorithm "],["tokenization.html", "3 Tokenization", " 3 Tokenization 3.0.1 N-Grams Language Models assign probabilities to sequence of words \\(P(w_1, w_2, ..., w_n)\\) Simplify the calculation using chain rule \\(P(w_1, w_2, ..., w_n) = P(w_1) \\times P(w_2 | w_1)..... \\times P(w_n | w_1 w_2 .. w_{n-1})\\) \\(P(w_1, w_2, ..., w_n) = \\prod P(w_i | w_{1:i-1})\\) Joint probability can be expressed as a product of conditional probabilities Probability of a word given historical context \\(P(w_n | h)\\) N-gram refers to a sequence of n words N-gram model makes the Markov assumption \\(P(w | h)\\) can be approximated using just the last n-1 words For example in case of a bigram model \\(P(w_n | h) \\approx P(w_n | w_{n-1})\\) Estimate the probabilities using Maximum Likelihood Relative frequency \\(P(w_n | w_{n-1}) = {P(w_{n-1}, w_n) \\over \\sum_k P(w_{n-1}, w_k)}\\) \\(P(w_n | w_{n-1}) = P(w_{n-1}, w_n) / P(w_{n-1})\\) BOS and EOS tokens to handle the edge cases N-gram models apt at capturing syntactic features (noun-verb-adj etc) To avoid numerical underflow, overflow problems use log probabilities \\(p_1 \\times p_2 = \\exp(\\log p_1 + \\log p_2)\\) 3.0.2 Perplexity Inverse probability normalized by the length of sequence \\(PP(W) = P(w_1 w_2 ... w_n)^{ - {1 \\over n}}\\) \\(PP(W) = \\sqrt[n]{\\prod 1 / P(w_i | w_{1:i-1})}\\) Higher the conditional probability, lower is the perplexity Weighted average branching factor Branching factor refers to the number of possible words that can follow a particluar word Perplxity of LMs comparable only if they use same vocabulary 3.0.3 Perplexity and Entropy Entropy is a measure of information Number of bits it takes to encode information (log base 2) \\(H(x) = - \\sum p \\log (p)\\) Entropy Rate: Entropy // Seq Length LMs can potentially consider infinite sequence length \\(H(W) = - \\lim_{n \\to \\infty} {1 \\over n} \\sum p(w_{1:n}) \\log(p_{1:n})\\) \\(H(W) \\approx - {1 \\over n} \\log p(w_{1:n})\\) \\(P(W) = 2^{H(W)}\\) 3.0.4 Unknown Words If probability of a word is zero, the perplexity is not defined. Unknown words or OOV words (out of vocab) Handle via pre-processing token Replace rare words with this token in training corpus LMs can achieve lower perplexity by selecting smaller vocab size 3.0.5 Smoothing Avoid assigning zero probabilities to unseen sequences Laplace Smoothing Add smoothing constants while calculating relative frequencies 1 to numerator V to denominator, V is the vocab size to ensure that probabilities sum up to 1 \\(P(w_i) = (w_i + 1) / (N + V)\\) \\(P(w_i | w_n) = \\text{count}(w_i, w_n) + 1 / \\text{count}(w_n) + V\\) Discount some probability mass from seen phrases and save it for unseen phrases Generalization to “Add - k” smoothing Back-off Use shorter sequences in case not enough support for full context Trigram if evidence is sufficent, otherwise use bigram Interpolation Mix the probability estimates from all n-grams \\(P(w_1 | w_2 w_3) = \\lambda_1 P(w_1) + \\lambda_2 P(w_1 | w_2) + + \\lambda_3 P(w_1 | w_2 w_3)\\) Kneser-Ney Smoothing Absolute discounting \\(P(w_1 | w_2) = C(w_1 w_2) - d / \\sum c(w_2) + \\lambda P(w_1)\\) 3.0.6 Efficiency Reduce memory footprint Quantization for probabilities Reverse Tries for N-grams String Hashing Bloom filters Stupid Backoff "],["vectors.html", "4 Vectors", " 4 Vectors 4.0.1 Lexical Semantics Issues that make it harder for syntactic models to scale well Lemmas and word forms (sing vs sang vs sung are infinitive forms of sing) Word Sense Disambiguation (mouse animal vs mouse hardware) Synonyms with same propositional meaning (couch vs sofa) Word Relatedness (coffee vs cup) Semantic Frames (A buy from B vs B sell to A) Connotation (affective meaning) 4.0.2 Vector Semantics Represent words using vectors called “embeddings” Derived from co-occurance matrix Document Vectors Term-Document Matrix |V| X |D| Dimension Count of times a word shows up in a document Vector of the document in |V| dimension space Used for informational retrieval Vector Space Model Word Vectors Term-Term Matrix |V| x |V| dimension Number of times a word and context word show up in the same document Word-Word co-occurance matrix Sparsity is a challenge Cosine Distance Normalized Dot Product Normalized by the l2-norm, to control for vector size \\(\\cos \\theta = a.b / |a||b|\\) 1 if vectors are in the same direction -1 if vectors are in opposite direction 0 if vectors are perpendicular For nomrlized vectors, it’s directly related to euclidean distance \\(|a - b|^2 = |a|^2 + |b|^2 - 2|a||b|\\cos\\theta = 2(1 - \\cos \\theta)\\) 4.0.3 TF-IDF Term Frequency Frequency of word t in document d \\(tf_{t,d} = \\text{count}(t,d)\\) Smooth TF \\(tf_{t,d} = \\log(1 + \\text{count}(t,d))\\) Document Frequency Number of documents in which term t appears \\(df_t\\) Inverse Document Frequency \\(idf_t = \\log(N / df_t)\\) TF-IDF \\(w_{t,d} = tf_{t,d} \\times idf_t\\) 4.0.4 PMI Ratio How often to x and y actually co-occur? How often will x and y co-occur if they were independent? \\(I(x,y) = \\log ({P(x,y) \\over P(x)P(y)})\\) Ranges from negative to positive infinity Positive PMI max(0, PMI) 4.0.5 Vector Representation For a given word T Term-Document Matrix Each word vector has |D| dimensions Each cell is weighted using TF-IDF logic Document Vector Average of all word vecotrs appearing in the document Similarity is calculated by cosine distance 4.0.6 Word2Vec Tf-IDF and PMI generate sparse vectors Need for dense and more efficient representation of words Static EMbeddings Skipgram with Negative Sampling Contextual Embeddings Dynamic embedding for each word Changes with context (ex - positional embedding) Self Supervised Learning 4.0.7 Skipgram Algorithm Treat tatget workd and neighbouring context word as positive samples (Window) Randomly sample other words from vocab as negative samples Use FFNN / Logistic Regression train a classifier Use the learned weights as embeddings Positive Examples Context Window of Size 2 All words +-2 from the given word Negative Examples Unigram frequency Downweighted to avoid sampling stop words frequently \\(P_{ij} \\propto f_{ij}^{0.75}\\) Classifier Maximize the similarity to positive samples Minimize the similarity to negative samples \\(L_{CE} = \\log P(+ | w,C_+) - \\sum \\log P(- | w,C_-)\\) \\(L_{CE} = \\log \\sigma(w . C_+) - \\sum \\log \\sigma(-w . C_-)\\) Use SGD to update w Each word has two separate embeddings target (when is shows up as w) context (when it shows up as c) Final embedding is the sum of the two 4.0.8 Enhancements Unknown / OOV words Use subwords models like FastText n-grams on characters GloVe Global vectors Ratios of probabilities form word-word co-occurance matrix Similarity \\(a:b :: a&#39;:b&#39;\\) \\(b&#39; = \\arg \\min \\text{distance}(x, b - a + a&#39;)\\) Bias Allocation Harm Unfair to different groups father-doctor, mother - housewife Representational Harm Wrong association for marginal groups African-american names to negative sentiment words "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
